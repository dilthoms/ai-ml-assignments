{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a neural network with a single hidden layer\n",
    "\n",
    "In this assignment you will learn to implement a neural network with a single hidden layer and apply it on a toy dataset. You will be using this neural networks to perform binary classification. In a binary classification problem, each input belongs to one of two classes, say 0 or 1, and the goal is to predict the correct class of each input. This assignment will help you understand how backpropagation is done in neural networks. This notebook is based on the material from: <br/>\n",
    "http://cs231n.github.io/neural-networks-case-study/ <br/>\n",
    "https://cs.stanford.edu/people/karpathy/cs231nfiles/minimal_net.html <br/>\n",
    "contribution from <a href=\"https://github.com/Abhi001vj\"> Abhilash V Jayakumar </a> \n",
    "\n",
    "\n",
    "#### Instructions\n",
    "-  Do not use any additional libraries other than what is already specified below\n",
    "-  Do not use loops in your code, use numpy for vectorized implementation\n",
    "-  Do not edit \"Expected output\" markdown cells\n",
    "-  Use https://forum.mulearn.org/ for asking any questions you may have about this assignment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import Libraries\n",
    "For this assignment we will import the following libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Checking out the repository to make it work with colab\n",
    "!git clone https://github.com/<your-github-username>/ai-ml-assignments.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append('ai-ml-assignments')\n",
    "from utils import utils\n",
    "from utils.utils import sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Creating the data\n",
    "Let us create the same simple dataset that we used for logistic regression assignment and visualise it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 160
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "12oCRiBvzCa5",
    "outputId": "942de7f4-05d5-4b02-f4e6-de206314d2c3"
   },
   "outputs": [],
   "source": [
    "X_train,yhat_train,X_test,yhat_test = utils.create_simple_dataset()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3. Data representation and parameters\n",
    "Assuming there are $m$ training samples, the pair $(x^{(i)}, \\hat{y}^{(i)})$ denote the $i^{th}$ training sample. Each training sample consists of $n$ features denoted by $x^{(i)}$ and the correponding class label $\\hat{y}^{(i)}$. For this dataset, we use two features, hence $n=2$. For each training sample, the features $x^{(i)}$ can be thought of as an $n\\times1$ column vector. \n",
    "\n",
    "For vectorized operations, we arrange the column vectors corresponding to all the features of the training samples in the form of a $n \\times m$ matrix $X$ and the ground truth class labels as a $1 \\times m$ row vector $\\hat{y}$. These are same as the conventions we followed in the logistic regression assignment.\n",
    "\n",
    "For this exercise, we will create a simple neural network having only one hidden layer with five neurons as shown below. The input layer has two nodes since the number of features we use is $n=2$. The output layer has a single neuron since this neural network is used for binary classification. Let $n_x$, $n_h$, and $n_y$ denote the number of nodes in the input layer, the hidden layer, and the output layer respectively. In this exercise, $n_x = 2$, $n_h = 5$ and $n_y = 1$.\n",
    "<img src=\"nn.png\" style=\"width:600px;height:300px;\">\n",
    "\n",
    "Each edge of the neural network has an associated weight. The weights corresponding to the edges between the input layer and the hidden layer can be represented as a $n_h \\times n_x$ matrix $W_1$. Similarly, a $n_y \\times n_h$ matrix $W_2$ represents the weights corresponding to the edges between the hidden layer and the output layer. The bias parameters for the nodes of the hidden layer and output layer are denoted by $b_1$ and $b_2$ respectively.\n",
    "\n",
    "### 4. Create Weight and Bias variables\n",
    "Use the cell below to create the weight parameters $W_1$ and $W_2$ and initialise them with small random values. Similarly create bias parameters $b_1$ and $b_2$ and initialise them with zeros.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "RHCCPRM96nc5"
   },
   "outputs": [],
   "source": [
    "def create_params(n_x, n_h, n_y):\n",
    "    # Create the weight and bias parameters\n",
    "    \n",
    "    #Arguments\n",
    "    #    n_x : size of the input layer\n",
    "    #    n_h : size of the hidden layer\n",
    "    #    n_y : size of the output layer\n",
    "    \n",
    "    # Return values\n",
    "    #    parameter dictionary that contains the randomly generated weights and parameters\n",
    "    \n",
    "    \n",
    "    # Set a seed for the random number generator to reproduce the results\n",
    "    np.random.seed(2)\n",
    "    \n",
    "    # The parameters will be stored in a python dictionary for convenience.\n",
    "    # Initialize the dictionary\n",
    "    param_dict = {}\n",
    "    \n",
    "    # Create a small set of random values as the weight and bias parameters\n",
    "    param_dict['W1'] = np.random.randn(n_h, n_x) * 0.01\n",
    "    param_dict['b1'] = np.zeros((n_h,1))\n",
    "    param_dict['W2'] = np.random.randn(n_y, n_h) * 0.01\n",
    "    param_dict['b2'] = np.zeros((n_y,1))\n",
    "    \n",
    "    return param_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "mtT7cPNV8nWi"
   },
   "outputs": [],
   "source": [
    "param_dict = create_params(2,5,1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Implement forward propagation\n",
    "Using the parameters $W_1$, $W_2$, $b_1$, and $b_2$, you have to implement the forward propgation and calculate the values for:<br/>\n",
    "$Z^{[1]} = W_1 \\cdot X + b_1$<br/>\n",
    "$A^{[1]} = tanh(Z^{[1]})$<br/>\n",
    "$Z^{[2]} = W_2 \\cdot A^{[1]} + b_2$<br/>\n",
    "$A^{[2]} = sigmoid(Z^{[2]})$ <br/>\n",
    "$L = -\\frac{1}{m} (\\hat Y log (A^{[2]}) + (1-\\hat Y)log(1-A^{[2]}))$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "vzfrt7LI1sFi"
   },
   "outputs": [],
   "source": [
    "def forward_propagate(X,yhat, param_dict):\n",
    "    # Implement forward propagation using the equations above without using any loops\n",
    "    \n",
    "    # Arguments:\n",
    "    #    X             : n features of m training samples represented as an nxm matrix\n",
    "    #    yhat          : the groundtruth values for the m training samples represented as an 1xm matrix\n",
    "    #    param_dict    : the parameter dictionary containing the weights and biases\n",
    "    #\n",
    "    # Return values:\n",
    "    #    loss          : the loss L calculated according to the formula above\n",
    "    #    cached_data   : dictionary storing the values of Z[1], A[1], Z[2], and A[2] calculated according \n",
    "    #                    to the formula above\n",
    "    #    \n",
    "    \n",
    "    \n",
    "    # We need the values of  Z[1], A[1], Z[2], and A[2] calculated during the forward pass later for the backward\n",
    "    # pass. Create a dictionary cached_data for storing the values.\n",
    "    cached_data = {}\n",
    "    \n",
    "    \n",
    "    # Calculate the values of Z[1], A[1], Z[2], and A[2] according to the formula above using vectorized operation\n",
    "    cached_data['Z1']  = \n",
    "    cached_data['A1']  = \n",
    "    cached_data['Z2']  = \n",
    "    cached_data['A2']  = \n",
    "    \n",
    "    # Compute the loss using the formula above\n",
    "    loss = \n",
    "    \n",
    "    return loss, cached_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let us test if the forward propagation is implemented correctly**\n",
    "\n",
    "We will use a few samples from the training set and check if the forward propagation calculates the correct values. Verify that the activation and loss values computed by the forward propagation you implemented matches the expected values below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Choose three samples of the training data for verification\n",
    "# Do not change these samples since the expected values are calculated for these samples.\n",
    "X = X_train[:,:3]\n",
    "yhat = yhat_train[:,:3]\n",
    "\n",
    "loss, cached_data = forward_propagate(X,yhat, param_dict)\n",
    "print (\"Actiavtion 1:\")\n",
    "print (cached_data['A1'])\n",
    "print (\"Activation 2:\")\n",
    "print (cached_data['A2'])\n",
    "print \"Loss:\"\n",
    "print loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table border=\"1\" class=\"dataframe\"> <thead>    <tr style=\"text-align: right;\"> <th>Expected output:</th></tr></thead> <tbody>\n",
    "<tr style=\"text-align: left;\"><td>Activation 1:<br/></td></tr>\n",
    "<tr style=\"text-align: left;\"><td>\n",
    "[[-0.00080023 -0.0032566   0.00072534]<br/>\n",
    "[-0.00043771 -0.02647377  0.01068074]<br/>\n",
    "[-0.00458275 -0.0109709   0.00095653]<br/>\n",
    "[-0.00152916  0.01059315 -0.00561624]<br/>\n",
    "[-0.00348696 -0.00437891 -0.00092487]]<br/>\n",
    "<td/></tr>\n",
    "<tr style=\"text-align: left;\"><td>Activation 2:<br/></td></tr>\n",
    "<tr style=\"text-align: left;\"><td>\n",
    "[[ 0.49999549  0.49980716  0.50007676]]\n",
    "<td/></tr>\n",
    "<tr style=\"text-align: left;\"><td>Loss:<br/></td></tr>\n",
    "<tr style=\"text-align: left;\"><td>0.693221593345<br/></td></tr>\n",
    "</tbody>  </table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Implement backward propagation\n",
    "Now comes the more difficult backward propagation where you have to compute the derivative with respect to the loss function. You need to implement the following in a vectorised manner:\n",
    "\n",
    "$dZ^{[2]}  = A^{[2]} - \\hat Y$<br/>\n",
    "$dW^{[2]}  = \\frac{1}{m} dZ^{[2]} \\cdot {A^{[1]}}^{T}$<br/>\n",
    "$db^{[2]}  = \\frac{1}{m} \\sum dZ^{[2]}$<br/>\n",
    "$dZ^{[1]}  = (W_2^T \\cdot dZ^{[2]}) * ( 1 - {A^{[1]}}^2)$<br/>\n",
    "$dW^{[1]}  = \\frac{1}{m} dZ^{[1]} \\cdot  X^T$<br/>\n",
    "$db^{[1]}  = \\frac{1}{m} \\sum dZ^{[1]}$<br/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "1P8XNQqP6slC"
   },
   "outputs": [],
   "source": [
    "def backward_propagate(X,yhat, param_dict, cached_data):\n",
    "    # Implement backward propagation using the equations above without using any loops\n",
    "    \n",
    "    # Arguments:\n",
    "    #    X    : n features of m training samples represented as an nxm matrix\n",
    "    #    yhat : the groundtruth values for the m training samples represented as an 1xm matrix\n",
    "    #    param_dict    : the parameter dictionary containing the weights and biases\n",
    "    #\n",
    "    # Return values:\n",
    "    #    dW1, db1, dW2, db2   : derivative of loss with respect to the weights and biases\n",
    "    \n",
    "    # Determine the number of trainining samples m from the shape of X\n",
    "    m = \n",
    "    \n",
    "    # Calculate the following using the equations above without using any loops: \n",
    "    #    dZ2 = dL/dZ2, \n",
    "    #    dW2 = dL/dW2,\n",
    "    #    db2 = dL/db2\n",
    "    #    dZ1 = dL/dZ1\n",
    "    #    dW1 = dL/dW1\n",
    "    #    db1 = dL/db1\n",
    "    \n",
    "    dZ2 = \n",
    "    dW2 = \n",
    "    db2 = \n",
    "    dZ1 = \n",
    "    dW1 = \n",
    "    db1 = \n",
    "    return dW1, db1, dW2, db2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let us test if the backward propagation is implemented correctly**\n",
    "\n",
    "Verify that the values computed by the backward propagation you implemented matches the expected values below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dW1, db1, dW2, db2 = backward_propagate(X,yhat, param_dict, cached_data)\n",
    "\n",
    "# assertions on size of dws and dbs\n",
    "print \"dW1:\"\n",
    "print(dW1)\n",
    "print \"db1:\"\n",
    "print(db1)\n",
    "\n",
    "print \"dW2:\"\n",
    "print(dW2)\n",
    "print \"db2:\"\n",
    "print(db2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table border=\"1\" class=\"dataframe\"> <thead>    <tr style=\"text-align: right;\"> <th>Expected output:</th></tr></thead> <tbody>\n",
    "<tr style=\"text-align: left;\"><td>dW1:<br/></td></tr>\n",
    "<tr style=\"text-align: left;\"><td>\n",
    "[[ -4.23821293e-04   3.09413235e-04]<br/>\n",
    " [ -1.75953603e-03   1.28494442e-03]<br/>\n",
    " [ -3.19189772e-05   2.33030631e-05]<br/>\n",
    " [  8.59036640e-04  -6.27176650e-04]<br/>\n",
    " [ -4.14289772e-04   3.02452398e-04]]<br/>\n",
    "<td/></tr>\n",
    "<tr style=\"text-align: left;\"><td>db1:<br/></td></tr>\n",
    "<tr style=\"text-align: left;\"><td>\n",
    "[[ -9.19302116e-04]<br/>\n",
    " [ -3.81815459e-03]<br/>\n",
    " [ -6.92420754e-05]<br/>\n",
    " [  1.86339492e-03]<br/>\n",
    " [ -8.98640161e-04]]<br/>\n",
    "<td/></tr>\n",
    "<tr style=\"text-align: left;\"><td>dW2:<br/></td></tr>\n",
    "<tr style=\"text-align: left;\"><td>\n",
    "[[ 0.00028873  0.00256119  0.00090601 -0.00108517  0.00030307]]<br/>\n",
    "<td/></tr>\n",
    "<tr style=\"text-align: left;\"><td>db2:<br/></td></tr>\n",
    "<tr style=\"text-align: left;\"><td>[[-0.16670687]]<br/></td></tr>\n",
    "</tbody>  </table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Training the network\n",
    "With the forward and backward propagation implemented, we can now implement the fit function to train the network and  determine the values for the parameters. This is done using gradient descent rule: \n",
    "$ \\theta = \\theta - \\alpha \\cdot \\frac{\\partial L}{\\partial \\theta} $\n",
    "where $\\theta$ is a parameter that needs to be updated, $\\frac{\\partial L}{\\partial \\theta}$ is the gradient of  loss function $L$ with respect to $\\theta$ and $\\alpha$ is the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "qREtIE6Z6zuF"
   },
   "outputs": [],
   "source": [
    "def fit(X,yhat, lr = 0.5, numiter = 10000, display_loss = False):\n",
    "    # Implement the fit fuction that determines optimal values for the parameters w and b using gradient descent.\n",
    "    \n",
    "    # Arguments:\n",
    "    #    X            : n features of m training samples represented as an nxm matrix\n",
    "    #    yhat         : the groundtruth values for the m training samples represented as an 1xm matrix\n",
    "    #    lr           : learning rate for the gradient descent\n",
    "    #    numiter      : number of iterations for running the gradient descent \n",
    "    #    display_loss : whether to display loss\n",
    "    #\n",
    "    # Return values:\n",
    "    #    param_dict   : the weights and biases determined by the gradient descent\n",
    "    \n",
    "    # Determine the size of the input layer from the shape of X    \n",
    "    n_x = X.shape[0]\n",
    "    \n",
    "    # Create param_dict for storing the weights and biases which are initialized with random values\n",
    "    param_dict = create_params(n_x,5,1)\n",
    "    \n",
    "    # Iterate for numiter steps. This is the only place where you should be using a for loop.\n",
    "    for i in range(numiter):\n",
    "        # Calculate the values in cached_data and the loss using forward propagation\n",
    "        loss, cached_data = \n",
    "        \n",
    "        # Calculate the derivates using backward propagation\n",
    "        dw1,db1, dw2, db2 = \n",
    "        \n",
    "        # Update the parameters by moving a small step (lr) in the opposite direction of the gradient\n",
    "        param_dict['W1'] -= \n",
    "        param_dict['b1'] -= \n",
    "        param_dict['W2'] -= \n",
    "        param_dict['b2'] -= \n",
    "        \n",
    "        if i % 1000 == 0 and display_loss == True:\n",
    "            print(loss)\n",
    "    return param_dict\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let us train the model with the training data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "param_dict = fit(X_train,yhat_train, lr = 0.5, numiter = 10000, display_loss = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table border=\"1\" class=\"dataframe\"> <thead>    <tr style=\"text-align: right;\"> <th>Expected output:</th></tr></thead> <tbody>\n",
    "\n",
    "<tr style=\"text-align: left;\"><td>\n",
    "0.693091379275<br/>\n",
    "0.0694891717794<br/>\n",
    "0.0360562903116<br/>\n",
    "0.0278517255979<br/>\n",
    "0.0236036145033<br/>\n",
    "0.0208583558305<br/>\n",
    "0.018882674365<br/>\n",
    "0.017368419956<br/>\n",
    "0.0161582159196<br/>\n",
    "0.0151611938343<br/>\n",
    "<td/></tr>\n",
    "</tbody></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment with the learning rate**\n",
    "\n",
    "With lr = 0.5 you can see that the value of loss is drecreasing. \n",
    "To make the model learn faster, you can change the alpha, i.e., the learning rate, to a higher value (try what happens with alpha = 2), but if the learning rate is too high the loss may not converge (try what happens with alpha = 25). Experiment with the learning rate, observe the effect it has on the loss, and understand why it is happening."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Evaluate model \n",
    "Now that we have trained the model, let us evaluate the model on the test set. You need to implement the predict function which will take a set of inputs and the learnt parameters and return the predicted class (0 or 1) for each input. Refer the equiations in Section 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(X,param_dict):\n",
    "    # Implement the function to predict the output for a new input using the learned parameters.\n",
    "    \n",
    "    # Arguments:\n",
    "    #    X          : n features of m training samples represented as an nxm matrix\n",
    "    #    param_dict : dictionary that stores the weights and biases determined by the training\n",
    "    #\n",
    "    # Return values:\n",
    "    #    pred : prediction for each test sample\n",
    "    \n",
    "    \n",
    "    # Determine the activation for each input test sample from the hidden layer. Refer the equations\n",
    "    # in Section 5.\n",
    "    z1 = \n",
    "    a1 = \n",
    "    \n",
    "    # Determine the activation for each input test sample from the hidden layer. Refer the equations\n",
    "    # in Section 5.\n",
    "    z2 = \n",
    "    a2 = \n",
    "    \n",
    "    # Since we are using a binary classifer, the output has to be either a 0 or a 1, so you need\n",
    "    # to convert the final activation to a 0 or a 1 and return it as the prediction.\n",
    "    pred = \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing**\n",
    "\n",
    "Let us check that the predict function works as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print predict(X,param_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table border=\"1\" class=\"dataframe\"> <thead>    <tr style=\"text-align: right;\"> <th>Expected output:</th></tr></thead> <tbody>\n",
    "\n",
    "<tr style=\"text-align: left;\"><td>\n",
    "[[ 0.  1.  1.]]\n",
    "<td/></tr>\n",
    "</tbody></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation on the test set**\n",
    "\n",
    "Let us evaluate the accuracy of the trained model on the test set $\\texttt{X_test}$. Determine the accuracy by comparing with the groundtruth $\\texttt{yhat_test}$ without using for loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# evaluate test set accuracy\n",
    "predicted = predict(X_test,param_dict)\n",
    "accuracy = np.mean(predicted == yhat_test)\n",
    "print ('training accuracy: %.2f' % (accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table border=\"1\" class=\"dataframe\"> <thead>    <tr style=\"text-align: right;\"> <th>Expected output:</th></tr></thead> <tbody>\n",
    "\n",
    "<tr style=\"text-align: left;\"><td>\n",
    "training accuracy: 0.95\n",
    "<td/></tr>\n",
    "</tbody></table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the decision plane learned by the model\n",
    "Let us visualize the classifier learned by the neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "utils.vis_classifier_nn(X_test,yhat_test,param_dict,predict)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Week 3 assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
