{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "soMh_oOhlHD3"
   },
   "source": [
    "# 1. Setup kaggle cli and download dataset in google colab\n",
    "\n",
    "Since all data is lost when google colab session ends, the six steps given below will download dataset from kaggle and save you from the trouble of downloading the dataset everytime. The first two steps below have to be done manually the first time. After that the rest of the steps can be executed by running the three cells (steps 3-6) below. You have to run these three cells to download the dataset everytime you start a new session. \n",
    "  \n",
    "\n",
    "1. Download / create json credentials after creating an account in kaggle.  See https://github.com/Kaggle/kaggle-api for more details\n",
    "2. Upload the kaggle.json file to your google drive\n",
    "3. Run the script in the first cell below to download kaggle.json  to your colab environment\n",
    "4. It will ask you to click on a link and enter the verification code\n",
    "5. Install kaggle cli using pip install\n",
    "6. Download the dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "zUM5cK52lHD6",
    "outputId": "1dd48562-791c-464e-fbbb-d341f2dece70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download 100%.\n"
     ]
    }
   ],
   "source": [
    "# Code from https://medium.com/@move37timm/using-kaggle-api-for-google-colaboratory-d18645f93648\n",
    "# Create kaggle.json by following instructions at https://github.com/Kaggle/kaggle-api\n",
    "# Upload kaggle.json to google drive\n",
    "# Download kaggle.json to colab from the users google drive\n",
    "\n",
    "from googleapiclient.discovery import build\n",
    "import io, os\n",
    "from googleapiclient.http import MediaIoBaseDownload\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "drive_service = build('drive', 'v3')\n",
    "results = drive_service.files().list(\n",
    "        q=\"name = 'kaggle.json'\", fields=\"files(id)\").execute()\n",
    "kaggle_api_key = results.get('files', [])\n",
    "filename = \"/root/.kaggle/kaggle.json\"\n",
    "if not os.path.exists(os.path.dirname(filename)):\n",
    "  os.makedirs(os.path.dirname(filename))\n",
    "request = drive_service.files().get_media(fileId=kaggle_api_key[0]['id'])\n",
    "fh = io.FileIO(filename, 'wb')\n",
    "downloader = MediaIoBaseDownload(fh, request)\n",
    "done = False\n",
    "while done is False:\n",
    "    status, done = downloader.next_chunk()\n",
    "    print(\"Download %d%%.\" % int(status.progress() * 100))\n",
    "os.chmod(filename, 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 215
    },
    "colab_type": "code",
    "id": "9KPWpND0lHD-",
    "outputId": "032ba0d6-6147-4a85-e621-23fa7be6b912"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kaggle in /usr/local/lib/python2.7/dist-packages (1.5.3)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python2.7/dist-packages (from kaggle) (1.22)\n",
      "Requirement already satisfied: six>=1.10 in /usr/local/lib/python2.7/dist-packages (from kaggle) (1.11.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python2.7/dist-packages (from kaggle) (2019.3.9)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python2.7/dist-packages (from kaggle) (2.5.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python2.7/dist-packages (from kaggle) (2.18.4)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python2.7/dist-packages (from kaggle) (4.28.1)\n",
      "Requirement already satisfied: python-slugify in /usr/local/lib/python2.7/dist-packages (from kaggle) (3.0.2)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python2.7/dist-packages (from requests->kaggle) (2.6)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python2.7/dist-packages (from requests->kaggle) (3.0.4)\n",
      "Requirement already satisfied: text-unidecode==1.2 in /usr/local/lib/python2.7/dist-packages (from python-slugify->kaggle) (1.2)\n"
     ]
    }
   ],
   "source": [
    "# Install kaggle cli\n",
    "!pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 179
    },
    "colab_type": "code",
    "id": "kBU3yAwLlHEB",
    "outputId": "5ba5a097-4425-4c88-e6c4-5586191b1048"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading train.csv to /content\n",
      " 82% 60.0M/73.2M [00:00<00:00, 84.2MB/s]\n",
      "100% 73.2M/73.2M [00:00<00:00, 122MB/s] \n",
      "Downloading test.csv to /content\n",
      " 80% 39.0M/48.8M [00:00<00:00, 52.2MB/s]\n",
      "100% 48.8M/48.8M [00:00<00:00, 120MB/s] \n",
      "Downloading sample_submission.csv to /content\n",
      "  0% 0.00/235k [00:00<?, ?B/s]\n",
      "100% 235k/235k [00:00<00:00, 58.2MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Download the dataset for digit-recognizer chalenge\n",
    "!kaggle competitions download -c digit-recognizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Read data in pandas dataframe\n",
    "1. Check train and test csv files have been downloaded\n",
    "2. import pandas and numpy and create train and test dataframes from the respective csv files\n",
    "3. Inspect the dataframes\n",
    "4. Convert to numpy arrays for train, validation, and test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check train and test csv files exist\n",
    "!ls -ltr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read the csv files using pandas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df_tr = pd.read_csv('train.csv')\n",
    "df_te = pd.read_csv('test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Examine the contents of train.csv\n",
    "# Contains 28x28 pixel values and the corresponding digit label\n",
    "print (df_tr.info())\n",
    "df_tr.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Examine the contents of test.csv\n",
    "# Contains only the 28x28 pixel values without the corresponding digit label\n",
    "print (df_te.info())\n",
    "df_te.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Partition the training data into pixels (independent variable) and label (dependent variable)\n",
    "X = np.asarray(df_tr.drop('label',axis=1),dtype=np.float32).reshape(-1,28,28)\n",
    "yhat = np.asarray(df_tr['label'])\n",
    "\n",
    "# Generate random indices for creating a random validation set with 20% of the labelled data\n",
    "validx = (np.random.uniform(size=len(X)) <= 0.2)\n",
    "\n",
    "# Create training set (80% of the labelled data)\n",
    "X_trn = X[~validx]\n",
    "y_trn = yhat[~validx]\n",
    "\n",
    "# Create validation set (20% of the labelled data)\n",
    "X_val = X[validx]\n",
    "y_val = yhat[validx]\n",
    "\n",
    "# Create the test set\n",
    "X_tes = np.asarray(df_te,dtype=np.float32).reshape(-1,28,28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Visualize some of the data items\n",
    "1. import matplotlib\n",
    "2. Visualize the first few data items and verify the corresponding labels match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Concatenate nvis images horizontally and visualize it using matplot lib\n",
    "import matplotlib.pyplot as plt\n",
    "nvis = 12\n",
    "plt.imshow(np.concatenate(X_trn[:nvis],axis=1),cmap='gray',vmin=0,vmax=255)\n",
    "plt.show()\n",
    "\n",
    "# Print the corresponding labels to check they match\n",
    "y_trn[:nvis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Create a fully connected neural network in pytorch\n",
    "\n",
    "We follow the same steps as in [assignment 4](https://github.com/dilthoms/ai-ml-assignments/blob/master/AI-ML-Libs/sklearn-pytorch.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Create a class and define the layers in the __init__\n",
    "# and implement the forward propagation. Pytorch will automatically\n",
    "# calculate the backward propagation for you. \n",
    "\n",
    "class SingleHidden_NN(nn.Module):\n",
    "    '''\n",
    "    A Neural Network with a single hidden layer.\n",
    "    ''' \n",
    "    \n",
    "    # Create a constructor and define the layers and activations\n",
    "    def __init__(self, input_size,hidden_size,output_size):\n",
    "        '''\n",
    "        Arguments:\n",
    "            input_size  : The number of neurons in the input layer\n",
    "            hidden_size : The number of neurons in the hidden layer\n",
    "            output_size : The number of neurons in the output layer\n",
    "        '''\n",
    "        super(Digit_SingleHidden_NN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "      \n",
    "        # Define a pytorch linear layer that connects the input layer to the hidden layer\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
    "        # Define a pytorch linear layer that connects the hidden layer to the output layer\n",
    "        self.layer2 = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "\n",
    "         \n",
    "    def forward(self, x):\n",
    "      '''\n",
    "      Implement forward propagation with relu activation for the hidden layer.\n",
    "      Arguments:\n",
    "          x      : The input x\n",
    "      Returns:\n",
    "          output : The linear activation from the output layer\n",
    "      '''\n",
    "        output = self.layer2(F.relu(self.layer1(x.view(-1,self.input_size))))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a Dataset subclass for loading datasets in numpy arrays\n",
    "# See https://pytorch.org/tutorials/beginner/data_loading_tutorial.html#dataset-class\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class Numpy_XY_Dataset(Dataset):\n",
    "  '''\n",
    "  Dataset subclass for the MNIST digits dataset\n",
    "  '''\n",
    "  \n",
    "  def __init__(self,X,y):\n",
    "  '''\n",
    "  Create the independent and dependent variables\n",
    "  '''\n",
    "    super(DigitDataset,self).__init__()\n",
    "    self.X = X\n",
    "    self.y = y\n",
    "    assert(len(X)==len(y))\n",
    "    \n",
    "  def __len__(self):\n",
    "  '''\n",
    "  Return the size of the dataset\n",
    "  '''\n",
    "    return len(self.X)\n",
    "  \n",
    "  def __getitem__(self,idx):\n",
    "  '''\n",
    "  Return the data item at index idx\n",
    "  '''\n",
    "    return self.X[idx],self.y[idx]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Write the training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate predictions using the trained model\n",
    "# TODO: use dataloader for X_tes. Works for now since it is small\n",
    "with torch.no_grad():\n",
    "  _,res = torch.max(model(torch.from_numpy(X_tes)),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert the results to a pandas dataframe\n",
    "sub = pd.DataFrame({\"ImageId\":np.arange(1,28001),\"Label\":res})\n",
    "\n",
    "# Create the submission csv file from the dataframe\n",
    "sub.to_csv(\"sub.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Submit the csv file to kaggle using the kaggle api\n",
    "!kaggle competitions submit -c digit-recognizer -f sub.csv -m \"First attempt\"\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "kaggle-digits.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
